{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debiasing Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from debiasing.llm.models import OpenAICompletion\n",
    "from debiasing.llm.models import AntrophicCompletion\n",
    "from debiasing.llm.models import ModelConfigs\n",
    "from debiasing.llm.utils import LLMMessage\n",
    "from debiasing.llm.tools import (CALCULATOR,\n",
    "                                 GENDER_BIAS_CLASSIFIER_DESCRIPTION, \n",
    "                                 GENDER_BIAS_MULTI_LABEL_CLASSIFIER,\n",
    "                                 MultiLabelGenderBiasClassifier,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OpenAI and Antrophic chatcompletion models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "openai = OpenAICompletion()\n",
    "\n",
    "msgs = [\n",
    "    LLMMessage(\n",
    "        role=LLMMessage.MessageRole.USER,\n",
    "        content=\"What is the capital of France?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "text, response = openai.get_answer(\n",
    "    messages=msgs\n",
    ")\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the LLM are instantiziate using the model specify in `config.py`, however you can provide the `model_id` with the LLM provider option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Chile is Santiago.\n"
     ]
    }
   ],
   "source": [
    "# Ref: Antrophic models https://docs.anthropic.com/en/docs/about-claude/models\n",
    "# Es posible instanciar un modelo de AntrophicCompletion con el model_id de un modelo espec칤fico\n",
    "antrophic = AntrophicCompletion(model_id='claude-3-opus-20240229')\n",
    "\n",
    "msgs = [\n",
    "    LLMMessage(\n",
    "        role=LLMMessage.MessageRole.USER,\n",
    "        content=\"What is the capital of Chile?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "text, response = antrophic.get_answer(\n",
    "    messages=msgs\n",
    ")\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using a system prompt, you can pass when instantiate the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, I'm feeling as chipper as a squirrel on a caffeine high! How about you? Are you ready to tackle the day or just trying to figure out if that last slice of pizza is calling your name? 游꼣游땏\n",
      "--------------------------------------------------------------------------------\n",
      "I'm just a computer program, so I don't have feelings, but I'm here and ready to help you! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "llm_with_system_prompt = OpenAICompletion(\n",
    "    system=\"Your are a joker and you have a very funny talkative style\"\n",
    ")\n",
    "\n",
    "llm_without_system_prompt = OpenAICompletion(\n",
    "    system=None\n",
    ")\n",
    "\n",
    "msgs = [LLMMessage(role=LLMMessage.MessageRole.USER, content=\"How are u?\")]\n",
    "\n",
    "print(llm_with_system_prompt.get_answer(messages=msgs)[0])\n",
    "print('-' * 80)\n",
    "print(llm_without_system_prompt.get_answer(messages=msgs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm an experienced data scientist with over a decade of experience working on enterprise-scale analytics and machine learning projects. In my current role at a Fortune 500 company, I focus on leveraging data science to drive business value through predictive modeling, optimization, and advanced analytics solutions.\n",
      "\n",
      "I have extensive experience with Python, R, SQL, and various ML frameworks, and I've led numerous successful projects in areas like customer analytics, supply chain optimization, risk modeling, and marketing attribution. I'm passionate about translating complex technical concepts into actionable business insights and helping organizations make data-driven decisions.\n",
      "\n",
      "How can I help you today with your data science questions or challenges?\n",
      "--------------------------------------------------------------------------------\n",
      "Hi! I'm Claude, an AI assistant created by Anthropic. I aim to be direct and honest in my interactions. I'm happy to help with analysis, writing, math, coding and other tasks while being upfront about my capabilities and limitations. What would you like to discuss?\n"
     ]
    }
   ],
   "source": [
    "llm_with_system_prompt = AntrophicCompletion(\n",
    "    system=\"You are a seasoned data scientist at a Fortune 500 company.\"\n",
    ")\n",
    "\n",
    "llm_without_system_prompt = AntrophicCompletion(\n",
    "    system=None\n",
    ")\n",
    "\n",
    "msgs = [LLMMessage(role=LLMMessage.MessageRole.USER, content=\"Introduce yourself!\")]\n",
    "\n",
    "print(llm_with_system_prompt.get_answer(messages=msgs)[0])\n",
    "print('-' * 80)\n",
    "print(llm_without_system_prompt.get_answer(messages=msgs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You run in a loop of Thought, Action, PAUSE, Observation.\n",
    "At the end of the loop you output an Answer\n",
    "Use Thought to describe your thoughts about the question you have been asked.\n",
    "Use Action to run one of the actions available to you - then return PAUSE.\n",
    "Observation will be the result of running those actions.\n",
    "\n",
    "Your available actions are:\n",
    "\n",
    "calculate:\n",
    "e.g. calculate: 4 * 7 / 3\n",
    "Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
    "\n",
    "wikipedia:\n",
    "e.g. wikipedia: Django\n",
    "Returns a summary from searching Wikipedia\n",
    "\n",
    "simon_blog_search:\n",
    "e.g. simon_blog_search: Django\n",
    "Search Simon's blog for that term\n",
    "\n",
    "Always look things up on Wikipedia if you have the opportunity to do so.\n",
    "\n",
    "Example session:\n",
    "\n",
    "Question: What is the capital of France?\n",
    "Thought: I should look up France on Wikipedia\n",
    "Action: wikipedia: France\n",
    "PAUSE\n",
    "\n",
    "You will be called again with this:\n",
    "\n",
    "Observation: France is a country. The capital is Paris.\n",
    "\n",
    "You then output:\n",
    "\n",
    "Answer: The capital of France is Paris\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You run in a loop of Thought, Action, PAUSE, Observation.\n",
      "At the end of the loop you output an Answer\n",
      "Use Thought to describe your thoughts about the question you have been asked.\n",
      "Use Action to run one of the actions available to you - then return PAUSE.\n",
      "Observation will be the result of running those actions.\n",
      "\n",
      "Your available actions are:\n",
      "\n",
      "calculate:\n",
      "e.g. calculate: 4 * 7 / 3\n",
      "Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
      "\n",
      "wikipedia:\n",
      "e.g. wikipedia: Django\n",
      "Returns a summary from searching Wikipedia\n",
      "\n",
      "simon_blog_search:\n",
      "e.g. simon_blog_search: Django\n",
      "Search Simon's blog for that term\n",
      "\n",
      "Always look things up on Wikipedia if you have the opportunity to do so.\n",
      "\n",
      "Example session:\n",
      "\n",
      "Question: What is the capital of France?\n",
      "Thought: I should look up France on Wikipedia\n",
      "Action: wikipedia: France\n",
      "PAUSE\n",
      "\n",
      "You will be called again with this:\n",
      "\n",
      "Observation: France is a country. The capital is Paris.\n",
      "\n",
      "You then output:\n",
      "\n",
      "Answer: The capital of France is Paris\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling and Structured Outputs in OpenAI models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://openai.com/index/introducing-structured-outputs-in-the-api/\n",
    "\n",
    "\n",
    "The structured output mode is to order the LLM to complete given a user interaction a standard JSON form. \n",
    "There is two ways to use structured output using OpenAI model: \n",
    "\n",
    "1. Using via function calling mode as a new tool \n",
    "1. Dedicated structured output using `response_format`\n",
    "\n",
    "\n",
    "We will use the first way which its advantage is instance an LLM with multiple tools allowing to determine which tool used in different context.\n",
    "\n",
    "> _\"When Structured Outputs is turned on, the arguments generated by the model for function calls will reliably match the JSON Schema that you provide_\", [ref best practices section on function calling doc](https://platform.openai.com/docs/guides/function-calling#turn-on-structured-outputs-by-setting-strict-true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-Agm1g3ADmNamb3avRoIxubx8zGH86',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1734757752,\n",
       " 'model': 'gpt-4o-mini-2024-07-18',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': None,\n",
       "    'tool_calls': [{'id': 'call_duosqaFs67v0DZN0twJ0ewFi',\n",
       "      'type': 'function',\n",
       "      'function': {'name': 'calculator',\n",
       "       'arguments': '{\"operation\":\"divide\",\"number1\":28,\"number2\":3}'}}],\n",
       "    'refusal': None},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'tool_calls'}],\n",
       " 'usage': {'prompt_tokens': 308,\n",
       "  'completion_tokens': 25,\n",
       "  'total_tokens': 333,\n",
       "  'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
       "  'completion_tokens_details': {'reasoning_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'accepted_prediction_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0}},\n",
       " 'system_fingerprint': 'fp_0aa8d3e20b'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_bias_example = (\n",
    "    \"La karencita es tan tierna. \"\n",
    "    \"Debe ser profesora de preescolar \" \n",
    "    \"porque ense침a super bien sumas como 2 + 3. \"\n",
    "    \"Adem치s no creo que sea ingenieria \" \n",
    "    \"porque a las mujeres no les gusta eso\"\n",
    ")\n",
    "\n",
    "gender_bias_example2 = \"Policeman are rude and violent\"\n",
    "\n",
    "calculator_example = \"How much is 4 * 7 / 3?\"\n",
    "\n",
    "msgs = [\n",
    "    LLMMessage(\n",
    "        role=LLMMessage.MessageRole.USER,\n",
    "        content=calculator_example,\n",
    "        # content=gender_bias_example2\n",
    "    )\n",
    "]\n",
    "\n",
    "model_config = ModelConfigs(\n",
    "    max_tokens=400,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "openai = OpenAICompletion(configs=model_config, \n",
    "                        #   system=GENDER_BIAS_CLASSIFIER_DESCRIPTION,\n",
    "                          tools=[GENDER_BIAS_MULTI_LABEL_CLASSIFIER, CALCULATOR])\n",
    "\n",
    "text, response = openai.get_answer(msgs)\n",
    "\n",
    "print(\"text:\", text)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured output with function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-4o-2024-08-06',\n",
       " 'messages': [{'role': 'user',\n",
       "   'content': 'La karencita es tan tierna. Debe ser profesora de preescolar porque ense침a super bien sumas como 2 + 3. Adem치s no creo que sea ingenieria porque a las mujeres no les gusta eso'}],\n",
       " 'max_completion_tokens': 500,\n",
       " 'temperature': 0.0,\n",
       " 'tools': [{'type': 'function',\n",
       "   'function': {'name': 'gender_bias_classifier',\n",
       "    'description': 'Identify gender biases in text (if any) and specify in which specific part is the biases located.',\n",
       "    'strict': True,\n",
       "    'parameters': {'$defs': {'GenderBiasesEnum': {'description': 'Kind of gender biases to detect in the text\\nRef: Based on Table 2 from work https://arxiv.org/pdf/2201.08675 ',\n",
       "       'enum': ['GENERIC_PRONOUNS',\n",
       "        'STEREOTYPING_BIAS',\n",
       "        'SEXISM',\n",
       "        'EXCLUSIONARY_TERMS',\n",
       "        'SEMANTIC_BIAS'],\n",
       "       'title': 'GenderBiasesEnum',\n",
       "       'type': 'string'}},\n",
       "     'additionalProperties': False,\n",
       "     'description': 'Multi-label gender bias classifier tool. Identify possible gender biases within a text.',\n",
       "     'example': {'bias_label': ['GENERIC_PRONOUNS', 'SEXISM'],\n",
       "      'bias_text': ['A programmer must carry his laptop.',\n",
       "       'A programmer must carry his laptop with him to work.'],\n",
       "      'input': 'A programmer must carry his laptop with him to work.'},\n",
       "     'properties': {'bias_label': {'description': 'List of bias labels to detect in the text.',\n",
       "       'items': {'$ref': '#/$defs/GenderBiasesEnum'},\n",
       "       'title': 'bias_label',\n",
       "       'type': 'array'},\n",
       "      'bias_text': {'description': 'The specific part of the text that contains the gender bias which determines the bias_label.',\n",
       "       'items': {'type': 'string'},\n",
       "       'title': 'bias_text',\n",
       "       'type': 'array'}},\n",
       "     'required': ['bias_label', 'bias_text'],\n",
       "     'title': 'MultiLabelGenderBiasClassifier',\n",
       "     'type': 'object'}}},\n",
       "  {'type': 'function',\n",
       "   'function': {'name': 'calculator',\n",
       "    'description': 'A simple calculator tool that performs basic arithmetic operations and can be used to identify the result of the operation in a user request.',\n",
       "    'strict': False,\n",
       "    'parameters': {'additionalProperties': False,\n",
       "     'description': 'A simple calculator tool',\n",
       "     'properties': {'operation': {'title': 'Operation', 'type': 'string'},\n",
       "      'number1': {'title': 'Number1', 'type': 'integer'},\n",
       "      'number2': {'title': 'Number2', 'type': 'integer'}},\n",
       "     'required': ['operation', 'number1', 'number2'],\n",
       "     'title': 'Calcualtor',\n",
       "     'type': 'object'}}}],\n",
       " 'tool_choice': 'auto'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from debiasing.configs import settings\n",
    "\n",
    "\n",
    "tools = [GENDER_BIAS_MULTI_LABEL_CLASSIFIER, CALCULATOR]\n",
    "force_tool = False\n",
    "\n",
    "msgs = [\n",
    "    LLMMessage(\n",
    "        role=LLMMessage.MessageRole.USER,\n",
    "        content=\"La karencita es tan tierna. Debe ser profesora de preescolar porque ense침a super bien sumas como 2 + 3. Adem치s no creo que sea ingenieria porque a las mujeres no les gusta eso\"\n",
    "    )\n",
    "]\n",
    "\n",
    "parsed_messages = [\n",
    "    {\"role\": message.role.value, \"content\": message.content}\n",
    "    for message in msgs\n",
    "]\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {settings.OPENAI_API_KEY}\",\n",
    "}\n",
    "\n",
    "\n",
    "tool_config = (\n",
    "    [tool.openai_dump() for tool in tools] if tools else None\n",
    ")\n",
    "\n",
    "pprint.pprint(tool_config)\n",
    "\n",
    "body = {\n",
    "    \"model\": \"gpt-4o-2024-08-06\",\n",
    "    \"messages\": parsed_messages,\n",
    "    \"max_completion_tokens\": 500,\n",
    "    \"temperature\": 0.0,\n",
    "    **(\n",
    "        {\n",
    "            \"tools\": tool_config,\n",
    "            \"tool_choice\": \"required\" if force_tool else \"auto\",\n",
    "        }\n",
    "        if tool_config\n",
    "        else {}\n",
    "    )\n",
    "}\n",
    "body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-AglkYbtorMGVUmoa3uuf3VJb0N0az',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1734756690,\n",
       " 'model': 'gpt-4o-2024-08-06',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': None,\n",
       "    'tool_calls': [{'id': 'call_FxU2bOXoNtqO156GzLojOE0z',\n",
       "      'type': 'function',\n",
       "      'function': {'name': 'gender_bias_classifier',\n",
       "       'arguments': '{\"bias_label\": [\"STEREOTYPING_BIAS\", \"SEXISM\"], \"bias_text\": [\"Debe ser profesora de preescolar porque ense침a super bien sumas como 2 + 3\", \"no creo que sea ingenieria porque a las mujeres no les gusta eso\"]}'}},\n",
       "     {'id': 'call_8PVYuHHgAOXvwCRLLYSmEHro',\n",
       "      'type': 'function',\n",
       "      'function': {'name': 'calculator',\n",
       "       'arguments': '{\"operation\": \"addition\", \"number1\": 2, \"number2\": 3}'}}],\n",
       "    'refusal': None},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'tool_calls'}],\n",
       " 'usage': {'prompt_tokens': 319,\n",
       "  'completion_tokens': 108,\n",
       "  'total_tokens': 427,\n",
       "  'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
       "  'completion_tokens_details': {'reasoning_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'accepted_prediction_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0}},\n",
       " 'system_fingerprint': 'fp_d28bcae782'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = requests.post(\n",
    "    settings.OPENAI_CHAT_ENDPOINT,\n",
    "    headers=headers,\n",
    "    json=body,\n",
    "    timeout=settings.LLM_TIMEOUT\n",
    ")\n",
    "\n",
    "out.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "debiasing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
